{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 1, Module 4*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Project: Logistic Regression\n",
    "\n",
    "Do you like burritos? ðŸŒ¯ You're in luck then, because in this project you'll create a model to predict whether a burrito is `'Great'`.\n",
    "\n",
    "The dataset for this assignment comes from [Scott Cole](https://srcole.github.io/100burritos/), a San Diego-based data scientist and burrito enthusiast. \n",
    "\n",
    "## Directions\n",
    "\n",
    "The tasks for this project are the following:\n",
    "\n",
    "- **Task 1:** Import `csv` file using `wrangle` function.\n",
    "- **Task 2:** Conduct exploratory data analysis (EDA), and modify `wrangle` function .\n",
    "- **Task 3:** Split data into feature matrix `X` and target vector `y`.\n",
    "- **Task 4:** Split feature matrix `X` and target vector `y` into training and test sets.\n",
    "- **Task 5:** Establish the baseline accuracy score for your dataset.\n",
    "- **Task 6:** Build `model_logr` using a pipeline that includes three transfomers and `LogisticRegression` predictor. Train model on `X_train` and `X_test`.\n",
    "- **Task 7:** Calculate the training and test accuracy score for your model.\n",
    "- **Task 8:** Create a horizontal bar chart showing the 10 most influencial features for your  model. \n",
    "- **Task 9:** Demonstrate and explain the differences between `model_lr.predict()` and `model_lr.predict_proba()`.\n",
    "\n",
    "**Note** \n",
    "\n",
    "You should limit yourself to the following libraries:\n",
    "\n",
    "- `category_encoders`\n",
    "- `matplotlib`\n",
    "- `pandas`\n",
    "- `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n",
    "    !pip install category_encoders==2.*\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = DATA_PATH + 'burritos/burritos.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Wrangle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(filepath):\n",
    "    # Import w/ DateTimeIndex\n",
    "    df = pd.read_csv(filepath, parse_dates=['Date'],\n",
    "                     index_col='Date')\n",
    "    df = df.sort_index()\n",
    "    # Drop unrated burritos\n",
    "    df.dropna(subset=['overall'], inplace=True)\n",
    "    \n",
    "    # Derive binary classification target:\n",
    "    # We define a 'Great' burrito as having an\n",
    "    # overall rating of 4 or higher, on a 5 point scale\n",
    "    df['Great'] = (df['overall'] >= 4).astype(int)\n",
    "    \n",
    "    # Drop high cardinality categoricals\n",
    "    df = df.drop(columns=['Notes', 'Location', 'Address', 'URL', 'Neighborhood'])\n",
    "#     Fix odd dates.  who knows what else is off. ðŸ¤·\n",
    "    df = df.truncate(before=pd.Timestamp('2016-01-01'))\n",
    "    \n",
    "    #DataFrame.truncate(before=None, after=None, axis=None, copy=True)\n",
    "    \n",
    "    #convert values to 1's and 0's     #make all x's true\n",
    "    df.replace(to_replace=\"Yes\",value=1,inplace=True)\n",
    "    df.replace(to_replace='No',value=0,inplace=True)\n",
    "    df = df.fillna(0)\n",
    "    df = df.replace(['X', 'x'], 1)\n",
    "    \n",
    "    # I dont want to drop rare ingredients because it might just be they are key to high ratings.\n",
    "    rarities = [col for col in df.select_dtypes('int').columns if df[col].sum() < (421*.02)]\n",
    "    print(rarities) # ['NonSD', 'Fish', 'Tomato', 'Bell peper', 'Carrots', 'Cabbage', 'Salsa.1', 'Taquito', 'Pineapple', 'Ham', 'Chile relleno', 'Nopales', 'Lobster', 'Egg', 'Mushroom', 'Bacon', 'Sushi', 'Corn', 'Zucchini']\n",
    "    #df.drop(columns= rarities, inplace=True)\n",
    "    \n",
    "    # Queso is a float\n",
    "    df.Queso = df.Queso.astype(int)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Use the above `wrangle` function to import the `burritos.csv` file into a DataFrame named `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NonSD', 'Fish', 'Tomato', 'Bell peper', 'Carrots', 'Cabbage', 'Salsa.1', 'Taquito', 'Pineapple', 'Ham', 'Chile relleno', 'Nopales', 'Lobster', 'Egg', 'Mushroom', 'Bacon', 'Sushi', 'Corn', 'Zucchini']\n"
     ]
    }
   ],
   "source": [
    "#filepath = DATA_PATH + 'burritos/burritos.csv'\n",
    "df = wrangle(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2016-01-18    3.80\n",
       "2016-01-24    3.75\n",
       "2016-01-24    3.00\n",
       "2016-01-24    3.00\n",
       "2016-01-27    4.20\n",
       "Name: overall, dtype: float64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['overall'].head()  # Press ctrl + / to comment/uncomment a block of selected code or a single unselected line.\n",
    "# Date\n",
    "# 2011-05-16    2.00\n",
    "# 2015-04-20    4.80\n",
    "# 2016-01-18    3.80\n",
    "# 2016-01-24    3.75\n",
    "# 2016-01-24    3.00\n",
    "# Name: overall, dtype: float64\n",
    "\n",
    "# Date\n",
    "# 2016-01-18    3.80\n",
    "# 2016-01-24    3.75\n",
    "# 2016-01-24    3.00\n",
    "# 2016-01-24    3.00\n",
    "# 2016-01-27    4.20\n",
    "# Name: overall, dtype: float64\n",
    "\n",
    "# Fixed odd dates.  They could have been entry mistakes but I dont know that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c# df['avg_rating'] = (df.Yelp + df.Google)/2\n",
    "# df.avg_rating #.round(0)\n",
    "# df.columns\n",
    "#># Index(['Burrito', 'Yelp', 'Google', 'Chips', 'Cost', 'Hunger', 'Mass (g)',\n",
    "#        'Density (g/mL)', 'Length', 'Circum', 'Volume', 'Tortilla', 'Temp',\n",
    "#        'Meat', 'Fillings', 'Meat:filling', 'Uniformity', 'Salsa', 'Synergy',\n",
    "#        'Wrap', 'overall', 'Rec', 'Reviewer', 'Unreliable', 'NonSD', 'Beef',\n",
    "#        'Pico', 'Guac', 'Cheese', 'Fries', 'Sour cream', 'Pork', 'Chicken',\n",
    "#        'Shrimp', 'Fish', 'Rice', 'Beans', 'Lettuce', 'Tomato', 'Bell peper',\n",
    "#        'Carrots', 'Cabbage', 'Sauce', 'Salsa.1', 'Cilantro', 'Onion',\n",
    "#        'Taquito', 'Pineapple', 'Ham', 'Chile relleno', 'Nopales', 'Lobster',\n",
    "#        'Queso', 'Egg', 'Mushroom', 'Bacon', 'Sushi', 'Avocado', 'Corn',\n",
    "#        'Zucchini', 'Great', 'avg_rating'],\n",
    "#       dtype='object')\n",
    "# df['NonSD'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Synergy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Cheese'].sum() #159\n",
    "#c# objCol = df.select_dtypes('float64').columns\n",
    "# objCol\n",
    "#># Index(['Yelp', 'Google', 'Cost', 'Hunger', 'Mass (g)', 'Density (g/mL)',\n",
    "#        'Length', 'Circum', 'Volume', 'Tortilla', 'Temp', 'Meat', 'Fillings',\n",
    "#        'Meat:filling', 'Uniformity', 'Salsa', 'Synergy', 'Wrap', 'overall',\n",
    "#        'avg_rating'],\n",
    "#       dtype='object')\n",
    "\n",
    "#c# df.dtypes.unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Burrito</th>\n",
       "      <th>Yelp</th>\n",
       "      <th>Google</th>\n",
       "      <th>Chips</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Hunger</th>\n",
       "      <th>Mass (g)</th>\n",
       "      <th>Density (g/mL)</th>\n",
       "      <th>Length</th>\n",
       "      <th>Circum</th>\n",
       "      <th>...</th>\n",
       "      <th>Lobster</th>\n",
       "      <th>Queso</th>\n",
       "      <th>Egg</th>\n",
       "      <th>Mushroom</th>\n",
       "      <th>Bacon</th>\n",
       "      <th>Sushi</th>\n",
       "      <th>Avocado</th>\n",
       "      <th>Corn</th>\n",
       "      <th>Zucchini</th>\n",
       "      <th>Great</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-18</th>\n",
       "      <td>California</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.49</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-24</th>\n",
       "      <td>Carne asada</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-24</th>\n",
       "      <td>Carnitas</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.85</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-24</th>\n",
       "      <td>California</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0</td>\n",
       "      <td>5.45</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-27</th>\n",
       "      <td>California</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1</td>\n",
       "      <td>6.59</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Burrito  Yelp  Google  Chips  Cost  Hunger  Mass (g)  \\\n",
       "Date                                                                   \n",
       "2016-01-18  California    3.5     4.2      0  6.49     3.0       0.0   \n",
       "2016-01-24  Carne asada   0.0     0.0      0  5.25     2.0       0.0   \n",
       "2016-01-24     Carnitas   0.0     0.0      0  4.85     1.5       0.0   \n",
       "2016-01-24  California    3.5     3.3      0  5.45     3.5       0.0   \n",
       "2016-01-27   California   4.0     3.8      1  6.59     4.0       0.0   \n",
       "\n",
       "            Density (g/mL)  Length  Circum  ...  Lobster  Queso  Egg  \\\n",
       "Date                                        ...                        \n",
       "2016-01-18             0.0     0.0     0.0  ...        0      0    0   \n",
       "2016-01-24             0.0     0.0     0.0  ...        0      0    0   \n",
       "2016-01-24             0.0     0.0     0.0  ...        0      0    0   \n",
       "2016-01-24             0.0     0.0     0.0  ...        0      0    0   \n",
       "2016-01-27             0.0     0.0     0.0  ...        0      0    0   \n",
       "\n",
       "            Mushroom  Bacon  Sushi  Avocado  Corn  Zucchini  Great  \n",
       "Date                                                                \n",
       "2016-01-18         0      0      0        0     0         0      0  \n",
       "2016-01-24         0      0      0        0     0         0      0  \n",
       "2016-01-24         0      0      0        0     0         0      0  \n",
       "2016-01-24         0      0      0        0     0         0      0  \n",
       "2016-01-27         0      0      0        0     0         0      1  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Beef'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During your exploratory data analysis, note that there are several columns whose data type is `object` but that seem to be a binary encoding. For example, `df['Beef'].head()` returns:\n",
    "\n",
    "```\n",
    "0      x\n",
    "1      x\n",
    "2    NaN\n",
    "3      x\n",
    "4      x\n",
    "Name: Beef, dtype: object\n",
    "```\n",
    "\n",
    "**Task 2:** Change the `wrangle` function so that these columns are properly encoded as `0` and `1`s. Be sure your code handles upper- and lowercase `X`s, and `NaN`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct your exploratory data analysis here\n",
    "# And modify the `wrangle` function above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you explore the `'Burrito'` column of `df`, you'll notice that it's a high-cardinality categorical feature. You'll also notice that there's a lot of overlap between the categories. \n",
    "\n",
    "**Stretch Goal:** Change the `wrangle` function above so that it engineers four new features: `'california'`, `'asada'`, `'surf'`, and `'carnitas'`. Each row should have a `1` or `0` based on the text information in the `'Burrito'` column. For example, here's how the first 5 rows of the dataset would look.\n",
    "\n",
    "| **Burrito** | **california** | **asada** | **surf** | **carnitas** |\n",
    "| :---------- | :------------: | :-------: | :------: | :----------: |\n",
    "| California  |       1        |     0     |    0     |      0       |\n",
    "| California  |       1        |     0     |    0     |      0       |\n",
    "|  Carnitas   |       0        |     0     |    0     |      1       |\n",
    "| Carne asada |       0        |     1     |    0     |      0       |\n",
    "| California  |       1        |     0     |    0     |      0       |\n",
    "\n",
    "**Note:** Be sure to also drop the `'Burrito'` once you've engineered your new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct your exploratory data analysis here\n",
    "# And modify the `wrangle` function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Burrito.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Queso.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.value_counts # [421 rows x 59 columns]>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Burrito.value_counts()\n",
    "#n# First run: Name: Burrito, Length: 132, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Split Data\n",
    "\n",
    "**Task 3:** Split your dataset into the feature matrix `X` and the target vector `y`. You want to predict `'Great'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "target= 'Great'\n",
    "\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Split `X` and `y` into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`).\n",
    "\n",
    "- Your training set should include data from 2016 through 2017. \n",
    "- Your test set should include data from 2018 and later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016\n",
      "2026\n"
     ]
    }
   ],
   "source": [
    "print(df.index.year.min()) #2011 <- I removed 2 odd dates to fix this\n",
    "print(df.index.year.max()) # 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['old'] = (df.index.year < 2016) # messing around. returns bool vals\n",
    "#df['old'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_train = 2017\n",
    "min_test = 2018\n",
    "mask_train = (X.index.year <= max_train)# & (X.index.year < range_test) # dont need because I 'fixed' the index\n",
    "mask_test = (X.index.year >= min_test)\n",
    "\n",
    "X_train, y_train = X.loc[mask_train], y.loc[mask_train]\n",
    "X_test, y_test = X.loc[mask_test], y.loc[mask_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((419,), (419, 60))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((381, 60), (381,), (38, 60), (38,))"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train.shape #(381, 60)\n",
    "#X_test.shape #(38, 60)\n",
    "#X_train.head()\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Establish Baseline\n",
    "\n",
    "**Task 5:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy Score: 0.5826771653543307\n"
     ]
    }
   ],
   "source": [
    "baseline_acc = y_train.value_counts(normalize=True).max()\n",
    "print('Baseline Accuracy Score:', baseline_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Build Model\n",
    "\n",
    "**Task 6:** Build a `Pipeline` named `model_logr`, and fit it to your training data. Your pipeline should include:\n",
    "\n",
    "- a `OneHotEncoder` transformer for categorical features, \n",
    "- a `SimpleImputer` transformer to deal with missing values, \n",
    "- a [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transfomer (which often improves performance in a logistic regression model), and \n",
    "- a `LogisticRegression` predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python39\\lib\\site-packages\\category_encoders\\utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
      "  elif pd.api.types.is_categorical(cols):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('onehotencoder',\n",
       "                 OneHotEncoder(cols=['Burrito', 'Rec', 'Reviewer'],\n",
       "                               use_cat_names=True)),\n",
       "                ('simpleimputer', SimpleImputer()),\n",
       "                ('standardscaler', StandardScaler()),\n",
       "                ('logisticregression', LogisticRegression())])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logr = make_pipeline(OneHotEncoder(use_cat_names=True),\n",
    "                           SimpleImputer(strategy='mean'),\n",
    "                           StandardScaler(),\n",
    "                           LogisticRegression() )\n",
    "model_logr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Accuracy 0.9973753280839895\n"
     ]
    }
   ],
   "source": [
    "print('New Accuracy', model_logr.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Check Metrics\n",
    "\n",
    "**Task 7:** Calculate the training and test accuracy score for `model_lr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MAE: 0.9973753280839895\n",
      "Test MAE: 0.8157894736842105\n"
     ]
    }
   ],
   "source": [
    "training_acc = model_logr.score(X_train, y_train)\n",
    "test_acc = model_logr.score(X_test, y_test)\n",
    "\n",
    "print('Training MAE:', training_acc)\n",
    "print('Test MAE:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Communicate Results\n",
    "\n",
    "**Task 8:** Create a horizontal barchart that plots the 10 most important coefficients for `model_lr`, sorted by absolute value.\n",
    "\n",
    "**Note:** Since you created your model using a `Pipeline`, you'll need to use the [`named_steps`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) attribute to access the coefficients in your `LogisticRegression` predictor. Be sure to look at the shape of the coefficients array before you combine it with the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create your horizontal barchart here.\n",
    "coefficients = model_logr.named_steps['logisticregression'].coef_[0]\n",
    "features = model_logr.named_steps['onehotencoder'].get_feature_names()\n",
    "feat_imp = pd.Series(coefficients, index=features).sort_values(key=abs)\n",
    "feat_imp.tail(10).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more than one way to generate predictions with `model_lr`. For instance, you can use [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression) or [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression.predict_proba).\n",
    "\n",
    "**Task 9:** Generate predictions for `X_test` using both `predict` and `predict_proba`. Then below, write a summary of the differences in the output for these two methods. You should answer the following questions:\n",
    "\n",
    "- What data type do `predict` and `predict_proba` output?\n",
    "- What are the shapes of their different output?\n",
    "- What numerical values are in the output?\n",
    "- What do those numerical values represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write code here to explore the differences between `predict` and `predict_proba`.\n",
    "y_pred = model_logr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02429371, 0.97570629],\n",
       "       [0.99989866, 0.00010134],\n",
       "       [0.99698264, 0.00301736],\n",
       "       [0.00000255, 0.99999745],\n",
       "       [0.90329992, 0.09670008],\n",
       "       [0.0053384 , 0.9946616 ],\n",
       "       [0.00098913, 0.99901087],\n",
       "       [0.0252389 , 0.9747611 ],\n",
       "       [0.18169869, 0.81830131],\n",
       "       [0.92221817, 0.07778183],\n",
       "       [0.97054522, 0.02945478],\n",
       "       [0.96932463, 0.03067537],\n",
       "       [0.18692796, 0.81307204],\n",
       "       [0.02543154, 0.97456846],\n",
       "       [0.56057142, 0.43942858],\n",
       "       [0.13711954, 0.86288046],\n",
       "       [0.99999774, 0.00000226],\n",
       "       [0.99948719, 0.00051281],\n",
       "       [0.99916089, 0.00083911],\n",
       "       [0.93786338, 0.06213662],\n",
       "       [0.00063882, 0.99936118],\n",
       "       [0.97124696, 0.02875304],\n",
       "       [0.01662236, 0.98337764],\n",
       "       [0.93279665, 0.06720335],\n",
       "       [0.99353454, 0.00646546],\n",
       "       [0.99998443, 0.00001557],\n",
       "       [0.00001501, 0.99998499],\n",
       "       [0.04192722, 0.95807278],\n",
       "       [0.94059516, 0.05940484],\n",
       "       [0.68427892, 0.31572108],\n",
       "       [0.99993419, 0.00006581],\n",
       "       [0.00975381, 0.99024619],\n",
       "       [0.00095172, 0.99904828],\n",
       "       [0.95359567, 0.04640433],\n",
       "       [0.00036041, 0.99963959],\n",
       "       [0.05284069, 0.94715931],\n",
       "       [0.00162465, 0.99837535],\n",
       "       [0.00002274, 0.99997726]])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "y_pred_prob = model_logr.predict_proba(X_test)\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.97570629, 0.00010134, 0.00301736, 0.99999745, 0.09670008,\n",
       "       0.9946616 , 0.99901087, 0.9747611 , 0.81830131, 0.07778183,\n",
       "       0.02945478, 0.03067537, 0.81307204, 0.97456846, 0.43942858,\n",
       "       0.86288046, 0.00000226, 0.00051281, 0.00083911, 0.06213662,\n",
       "       0.99936118, 0.02875304, 0.98337764, 0.06720335, 0.00646546,\n",
       "       0.00001557, 0.99998499, 0.95807278, 0.05940484, 0.31572108,\n",
       "       0.00006581, 0.99024619, 0.99904828, 0.04640433, 0.99963959,\n",
       "       0.94715931, 0.99837535, 0.99997726])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob = model_logr.predict_proba(X_test)[:,1]\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give your written answer here:**\n",
    "\n",
    "The difference between ***predict** and predict_proba is that predict will give you output like 0,1. Whereas predict_proba will give you the probability value of y being 0 or 1.\n",
    "\n",
    "`model_logr.predict(X_test)`\n",
    "`array([1, 0, 0,...)`\n",
    "       \n",
    "***predict_proba** gives you the probabilities for the target in array form. The number of probabilities for each row is equal to the number of categories the in target variable.  Since this is binary its 2.  1 or 0.\n",
    "\n",
    "`model_logr.predict_proba(X_test)`\n",
    "\n",
    "`array([[0.02429371, 0.97570629],\n",
    "       [0.99989866, 0.00010134],\n",
    "       [0.99698264, 0.00301736],...])`\n",
    "\n",
    "The first # in each row is the probability that the output will be 0 and second is the probability of output being 1.\n",
    "\n",
    "If you only want the probability of getting the output either as 0 or 1, thereby getting only one result per category you can use the following code:\n",
    "\n",
    "`model_logr.predict_proba(X_test)[:,1]`  \n",
    "`#array([0.97570629, 0.00010134, 0.00301736,...])`\n",
    "\n",
    "Here, model_logr is the trained model. X_test is the dataset we made predictions for.\n",
    "\n",
    "Using [:,1] in the code will give you the probabilities of getting the output as 1. If you replace 1 with 0 in the above code, you will only get the probabilities of getting the output as 0.]]\n",
    "\n",
    "Now where to use predict and predict_proba.\n",
    "Predict can be used normally to get prediction values.\n",
    "\n",
    "But suppose you are working on cancer diagnosis problem and you want to be very sure with your results. So in that case you can use predict_proba which will give you class probability values and you can set some threshold like if predict_proba_value > .98 return class 1 else 0. So basically with the help of predit_proba we can set threshold as per our needs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_214_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
