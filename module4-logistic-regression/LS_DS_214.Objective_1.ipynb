{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Objective 01 - Implement a train-validate-test split\n",
    "## Overview\n",
    "In the previous module, we used a train-test split where we hold back a subset of the data to use for testing the model. When we train a model, we also need to evaluate the model. Recall that if we evaluate on the training data, we're not getting an accurate estimate of the true performance of the model. For this reason, we need to use test data that the model has not yet seen.\n",
    "\n",
    "But, sometimes it's useful to be able to have an intermediate step where the model can be evaluated without using the set-aside test set. This is where a validation set is useful. Consider the situation where we take a subset of our data and set it aside as the test set - we won't touch this data until we're ready to evaluate a final model.\n",
    "\n",
    "With the remaining data, we can divide it into training and validation sets. We then train the model on the training data and evaluate it on the validation data. Another advantage of using a validation set it that is can be used to tune the model or adjust the hyperparameters. Iterations of tuning and model fitting are used to find the final model, which is then evaluated using the test set.\n",
    "\n",
    "## Train-validate-test\n",
    "Some general definitions are:\n",
    "\n",
    "- training dataset: the sample of data used to fit the model\n",
    "- validation dataset: the sample of data used to evaluate the model and possibly to adjust the hyperparameters\n",
    "- testing dataset the sample of data used for final model testing; not to be used for anything other than testing so that the result is unbiased\n",
    "\n",
    "One last point to make is that sometimes you won't even have access to a test set! If you are participating in a Kaggle competition, for example, you will have training data and use the test data for your submission. The number of test submissions might be limited, or you might not want to make numerous test submissions just to evaluate your model.\n",
    "\n",
    "In this next section, we'll create our own train-validation-test data sets. We'll follow the guideline of using 60% for training, 20% for validation, and 20% for testing.\n",
    "\n",
    "## Follow Along\n",
    "As we haven't yet worked with the Iris dataset in this module, we'll start there. In the following example, we load the data and then separate out the feature (petal_width) and the target (petal_length). Having plotted this data earlier, we know there is a linear relationship between the petal width and length: the wider the petal, the greater the length. So we'll use a linear model to predict our target."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   sepal_length  sepal_width  petal_length  petal_width species\n0           5.1          3.5           1.4          0.2  setosa\n1           4.9          3.0           1.4          0.2  setosa\n2           4.7          3.2           1.3          0.2  setosa\n3           4.6          3.1           1.5          0.2  setosa\n4           5.0          3.6           1.4          0.2  setosa",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_length</th>\n      <th>sepal_width</th>\n      <th>petal_length</th>\n      <th>petal_width</th>\n      <th>species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\somer\\AppData\\Local\\Temp/ipykernel_900/3123036665.py:8: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n  X = x[:, np.newaxis]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import seaborn as sns \n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "display(iris.head() )\n",
    "\n",
    "x= iris['petal_width']\n",
    "X = x[:, np.newaxis]\n",
    "y = iris['petal_width']"
   ]
  },
  {
   "source": [
    "First, we'll hold back a subset of the data just for the test data. We'll do this with the scikit-learn utility; we'll call it something different from \"train\" so that we don't confuse it with the actual training data later.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the train_test_split utility\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create the \"remaining\" and test datasets\n",
    "X_remain, X_test, y_remain, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "source": [
    "Then we'll create a training set and validation set from the remaining data. We could have done this in one step but we're breaking it down here so it's easier to see that we removed a test subset and aren't accidentally going to use it for evaluation until we're ready to test.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data set samples: 90\nValidation data set samples: 30\nTest data set samples: 30\n"
     ]
    }
   ],
   "source": [
    "# Create the train and validation datasets\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_remain, y_remain, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print out sizes of train, validate, test datasets\n",
    "\n",
    "print('Training data set samples:', len(X_train))\n",
    "print('Validation data set samples:', len(X_val))\n",
    "print('Test data set samples:', len(X_test))"
   ]
  },
  {
   "source": [
    "Now we can fit our model and evaluate it on our validation set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Import the predictor and instantiate the class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instantiate the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Use the VALIDATION set for prediction\n",
    "y_predict = model.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_val, y_predict)"
   ]
  },
  {
   "source": [
    "Well, that's a pretty good model score (R-squared), which we expect because we know the Iris dataset has a strong linear trend between the petal width and petal length. Now would be the time to change any of the model hyperparameters and evaluate on the validation set again. We'll continue with the default model parameters for now; hyperparameter tuning is something that will be introduced in Sprint 2 of this unit.\n",
    "\n",
    "Now, let's use the test set we held back above."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Use the TEST set for prediction\n",
    "y_predict_test = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "\n",
    "r2_score(y_test, y_predict_test)"
   ]
  },
  {
   "source": [
    "The R-squared score is a little lower than it was for the validate set. But, if we were to run the model and test again, with a different random seed, the scores would be different and the test score might be higher.\n",
    "\n",
    "Challenge\n",
    "Using the same data set as in the example, try changing the random_state parameter to see how the validate and test model scores change.\n",
    "\n",
    "Additional Resources  \n",
    "What is the Difference Between Test and Validation Datasets?['https://machinelearningmastery.com/difference-test-validation-datasets/]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}