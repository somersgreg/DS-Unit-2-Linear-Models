{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective 04 - Use scikit learn to fit and interpret logistic regression models\n",
    "## Overview\n",
    "So far, we've looked at the function and coefficients used to fit a logistic regression. In this objective, we're going to go more into detail about how to use the scikit learn LogisticRegression predictor. We'll also cover how to fit this model using the two features in the dataset and how to interpret these results.\n",
    "\n",
    "## Follow Along\n",
    "Let's load the geyser data set we used earlier, and go through the steps to fit a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   duration  waiting   kind  kind_binary\n0     3.600       79   long            0\n1     1.800       54  short            1\n2     3.333       74   long            0\n3     2.283       62  short            1\n4     4.533       85   long            0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>duration</th>\n      <th>waiting</th>\n      <th>kind</th>\n      <th>kind_binary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.600</td>\n      <td>79</td>\n      <td>long</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.800</td>\n      <td>54</td>\n      <td>short</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.333</td>\n      <td>74</td>\n      <td>long</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.283</td>\n      <td>62</td>\n      <td>short</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4.533</td>\n      <td>85</td>\n      <td>long</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Import seaborn and load the data\n",
    "import seaborn as sns\n",
    "\n",
    "geyser = sns.load_dataset(\"geyser\")\n",
    "\n",
    "# Convert target labels to 0 or 1\n",
    "\n",
    "# Import the label encoder and instantiate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Create a new column with 0=long and 1=short class labels\n",
    "geyser['kind_binary'] = le.fit_transform(geyser['kind'])\n",
    "display(geyser.head())"
   ]
  },
  {
   "source": [
    "Now that we have our geyser class encoded, we can follow the usual model fitting procedure. First, we create the feature matrix and target array. Then we import the `LogisticRegression` model, instantiate the predictor class, and fit the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\somer\\AppData\\Local\\Temp/ipykernel_14548/871476010.py:8: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n  X = x[:, np.newaxis]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Import logistic regression predictor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Prepare the feature (we'll begin with one feature)\n",
    "import numpy as np\n",
    "\n",
    "x = geyser['duration']\n",
    "X = x[:, np.newaxis]\n",
    "\n",
    "# Assign the targert variable to y\n",
    "y = geyser['kind_binary']\n",
    "\n",
    "# Fit the model using the default parameters\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.94545455 1.         1.         0.94444444 1.        ]\nThe mean CV score is:  0.977979797979798\n"
     ]
    }
   ],
   "source": [
    "# Import the cross validation method\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Implement a cross-validation with k=5\n",
    "print(cross_val_score(model, X, y, cv=5))\n",
    "\n",
    "# Calculate the mean of the cross-validation scores\n",
    "score_mean = cross_val_score(model, X, y, cv=5).mean()\n",
    "print('The mean CV score is: ', score_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This model is pretty accurate. If we remember from earlier in this module, our baseline accuracy was 63%. We've improved over the baseline by a significant amount.\n",
    "\n",
    "Now, how much can we improve our model by using an additional feature in fitting our model? We still have the waiting column which is the amount of time that passes between eruptions. Let's add that feature to the feature matrix, fit the model, and calculate the cross-validation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1. 1. 1. 1. 1.]\nThe mean CV score is (two features):  1.0\n"
     ]
    }
   ],
   "source": [
    "# Create new feature matrix\n",
    "features = ['duration', 'waiting']\n",
    "X_two = geyser[features]\n",
    "\n",
    "# Fit the model using the default parameters\n",
    "model_two = LogisticRegression()\n",
    "model_two.fit(X_two, y)\n",
    "\n",
    "# Implement a cross-validation with k=5\n",
    "print(cross_val_score(model_two, X_two, y, cv=5))\n",
    "\n",
    "# Calculate the mean of the cross-validation scores\n",
    "score_mean = cross_val_score(model_two, X_two, y, cv=5).mean()\n",
    "print('The mean CV score is (two features): ', score_mean)"
   ]
  },
  {
   "source": [
    "The accuracy is perfect for this model. This is likely because the two classes have a very clear division. It's important to remember that not all data sets will be so easy to model with such accurate results!\n",
    "\n",
    "## Challenge\n",
    "For this challenge, try to plot the two features on the same plot. So instead of a plot with the feature on the x-axis and the class on the y axis, plot one feature on each axis. Are the two classes distinct as visualized on the plot? If you use two different colors for the classes, there should be a clear division between the classes. Think about where you would draw the decision boundary\n",
    "\n",
    "## Additional Resources\n",
    "- Scikit learn: Logistic Regression['https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html]"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}